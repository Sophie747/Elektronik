---
tags: []
aliases: []
subject: ["dic"]
source: ["Robert Vogl"]
reference: []
created: 7th November 2022
---

# Gradient Descent
## [[ML Klassifizierung|Logistische Regression]]

$$
\begin{align*}
cost = J &= \frac{1}{2m}\sum^{m}_{n=1}(h(x_{1}^{(n)})-y^{n})^{2} \\
\frac{\partial_{J}}{\partial_{\theta}} &= 
\end{align*}
$$
Die Kostenfunktion wird immer durch die Anzahl der Punkte dividiert


```mA
```
# Quellen
Gradient Descent Wiki



